{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Data analysis\n",
    "\n",
    "Data analysis will be conducted in this notebook and dataset used is the full dataset with all categorical features converted into model-understanding binary numerical columns. Several models will be implemented using GridSearch CV for the best performance. The discussion will emphasise on the most outstanding model selected to explain the fatality outcome based on the significant features found in the model. In closing, improvement and development for future analysis will be discussed in the last section of this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,recall_score,precision_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,cohen_kappa_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cas_age</th>\n",
       "      <th>n_occupants</th>\n",
       "      <th>total_units</th>\n",
       "      <th>cas_total</th>\n",
       "      <th>area_speed</th>\n",
       "      <th>cas_type_Driver</th>\n",
       "      <th>cas_type_Passenger</th>\n",
       "      <th>cas_type_Pedestrian</th>\n",
       "      <th>cas_type_Rider</th>\n",
       "      <th>cas_gender_Female</th>\n",
       "      <th>...</th>\n",
       "      <th>crash_type_Right Turn</th>\n",
       "      <th>crash_type_Roll Over</th>\n",
       "      <th>crash_type_Side Swipe</th>\n",
       "      <th>traf_ctrls_Give Way Sign</th>\n",
       "      <th>traf_ctrls_No Control</th>\n",
       "      <th>traf_ctrls_Other</th>\n",
       "      <th>traf_ctrls_Roundabout</th>\n",
       "      <th>traf_ctrls_Stop Sign</th>\n",
       "      <th>traf_ctrls_Traffic Signals</th>\n",
       "      <th>fatality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.889159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cas_age  n_occupants  total_units  cas_total  area_speed  \\\n",
       "0  34.000000          1.0            2          1          60   \n",
       "1  41.000000          1.0            2          1          60   \n",
       "2  39.889159          1.0            2          1          60   \n",
       "3  19.000000          1.0            2          7          60   \n",
       "4  48.000000          6.0            2          7          60   \n",
       "\n",
       "   cas_type_Driver  cas_type_Passenger  cas_type_Pedestrian  cas_type_Rider  \\\n",
       "0                1                   0                    0               0   \n",
       "1                1                   0                    0               0   \n",
       "2                1                   0                    0               0   \n",
       "3                1                   0                    0               0   \n",
       "4                1                   0                    0               0   \n",
       "\n",
       "   cas_gender_Female  ...  crash_type_Right Turn  crash_type_Roll Over  \\\n",
       "0                  1  ...                      0                     0   \n",
       "1                  1  ...                      1                     0   \n",
       "2                  0  ...                      0                     0   \n",
       "3                  0  ...                      0                     0   \n",
       "4                  0  ...                      0                     0   \n",
       "\n",
       "   crash_type_Side Swipe  traf_ctrls_Give Way Sign  traf_ctrls_No Control  \\\n",
       "0                      0                         0                      1   \n",
       "1                      0                         0                      1   \n",
       "2                      0                         0                      1   \n",
       "3                      0                         0                      1   \n",
       "4                      0                         0                      1   \n",
       "\n",
       "   traf_ctrls_Other  traf_ctrls_Roundabout  traf_ctrls_Stop Sign  \\\n",
       "0                 0                      0                     0   \n",
       "1                 0                      0                     0   \n",
       "2                 0                      0                     0   \n",
       "3                 0                      0                     0   \n",
       "4                 0                      0                     0   \n",
       "\n",
       "   traf_ctrls_Traffic Signals  fatality  \n",
       "0                           0         0  \n",
       "1                           0         0  \n",
       "2                           0         0  \n",
       "3                           0         0  \n",
       "4                           0         0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datas/final_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['fatality'], axis=1)\n",
    "y = data['fatality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X),columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model tuning\n",
    "\n",
    "We want to tune the parameters for each model along with different sampling techniques using StratifiedKFold to avoid overfitting data in extremely imbalanced dataset. The study of models includes logistic regression, MLP classifier and RusBoostClassifier and resampling techiques such as upsampling and downsampling will be performed during model training to optimise the prediction power due to underepresented class in the dataset. \n",
    "\n",
    "Due to highly imbalanced classes in the dataset, conventional score metrics such as accurracy, recall score and precision score might be ineffective in filtering the best parameters for model tuning. As such, score metric will be implemented in GridSearchCV is Cohen Kappa score, which is highly effective to measure how close an observed accuracy predicted in the selected model is compared with an expected accuracy. The closer the kappa score to 1, the better agreements between expected accuracy and observed accuracy in the model.\n",
    "\n",
    "For usefulness of the model, test set will be split from the original dataset and it will only be used for testing purpose in section 5.2 to ease overfitting issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# split test set for evaluation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "near_miss = NearMiss(sampling_strategy='auto',random_state=51)\n",
    "smote = SMOTE(sampling_strategy='auto',random_state=52)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=46,stratify=y,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3091,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PlotConfusionMatrix is to plot the confusion matrix and \n",
    "#normalised confusion matrix to visualise prediction for individual model\n",
    "def PlotConfusionMatrix(y_test,pred):\n",
    "    y_test_not_fatal=y_test.value_counts()[0]\n",
    "    y_test_fatal=y_test.value_counts()[1]\n",
    "    cfn_matrix = confusion_matrix(y_test,pred)\n",
    "    cfn_norm_matrix = np.array([[1.0 / y_test_not_fatal,1.0/y_test_not_fatal],[1.0/y_test_fatal,1.0/y_test_fatal]])\n",
    "    norm_cfn_matrix = cfn_matrix * cfn_norm_matrix\n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    sns.heatmap(cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    sns.heatmap(norm_cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
    "\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log__C': 0.0001, 'log__class_weight': {0: 1, 1: 10}, 'log__penalty': 'l2'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg1 = LogisticRegression(random_state=2)\n",
    "pipe_log_down = Pipeline([('sampling',near_miss), ('log',log_reg1)])\n",
    "log_reg_params = {\"log__penalty\": ['l1', 'l2'], 'log__C': [0.0001,0.001, 0.01, 0.1, 1],\n",
    "                 'log__class_weight':[{0:1,1:1},{0:1,1:10},{0:1,1:100},{0:1,1:1000}]}\n",
    "log_reg_down_sampling = GridSearchCV(pipe_log_down, param_grid=log_reg_params, scoring=make_scorer(cohen_kappa_score),\n",
    "                                   cv = skf)\n",
    "log_reg_down_sampling.fit(X_train,y_train)\n",
    "log_reg_down_sampling.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log__C': 0.1, 'log__class_weight': {0: 1, 1: 1}, 'log__penalty': 'l1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg2 = LogisticRegression(random_state=3)\n",
    "pipe_log_up = Pipeline([('sampling',smote), ('log',log_reg2)])\n",
    "log_reg_up_sampling = GridSearchCV(pipe_log_up, param_grid=log_reg_params, scoring=make_scorer(cohen_kappa_score),\n",
    "                                   cv = skf)\n",
    "log_reg_up_sampling.fit(X_train,y_train)\n",
    "log_reg_up_sampling.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2  MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_params = {\n",
    "    'nn__hidden_layer_sizes': [(50,50), (50,100), (100,)],\n",
    "    'nn__activation': ['tanh', 'relu'],\n",
    "    'nn__solver': ['sgd', 'adam'],\n",
    "    'nn__alpha': [ 0.001,0.01],\n",
    "    'nn__learning_rate': ['constant','adaptive']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nn__activation': 'tanh',\n",
       " 'nn__alpha': 0.01,\n",
       " 'nn__hidden_layer_sizes': (100,),\n",
       " 'nn__learning_rate': 'constant',\n",
       " 'nn__solver': 'sgd'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1 = MLPClassifier(max_iter=200,random_state=4)\n",
    "pipe_mlp_down = Pipeline([('sampling',near_miss), ('nn',mlp1)])\n",
    "mlp_down_sampling = GridSearchCV(pipe_mlp_down, mlp_params, n_jobs=-1, cv=skf,scoring=make_scorer(cohen_kappa_score))\n",
    "mlp_down_sampling.fit(X_train, y_train)\n",
    "mlp_down_sampling.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nn__activation': 'relu',\n",
       " 'nn__alpha': 0.001,\n",
       " 'nn__hidden_layer_sizes': (50, 50),\n",
       " 'nn__learning_rate': 'constant',\n",
       " 'nn__solver': 'adam'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(max_iter=200,random_state=5)\n",
    "pipe_mlp_up = Pipeline([('sampling',smote), ('nn',mlp2)])\n",
    "mlp_up_sampling = GridSearchCV(pipe_mlp_up, mlp_params, n_jobs=-1, cv=skf,scoring=make_scorer(cohen_kappa_score))\n",
    "mlp_up_sampling.fit(X_train, y_train)\n",
    "mlp_up_sampling.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 RUSBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rus__learning_rate': 0.1, 'rus__n_estimators': 300}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "rus = RUSBoostClassifier(sampling_strategy='auto',random_state=6)\n",
    "pipe_rus = Pipeline([ ('rus',rus)])\n",
    "rus_params = {'rus__learning_rate':[0.001,0.01,0.1,1],'rus__n_estimators':[100,200,300]}\n",
    "rus_clf = GridSearchCV(pipe_rus, param_grid=rus_params, scoring=make_scorer(cohen_kappa_score),\n",
    "                                   cv = skf)\n",
    "rus_clf.fit(X_train,y_train)\n",
    "rus_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model selection\n",
    "This section aims to select the best model out of all trained models in previous section. Test set split in section 5.1 will be used to measure how accurate the prediction is for each model. Score matrices will be performed include, Kappa score, recall score for class 1 to measure percentage of correctly predicted non-fatal casualties, recall score for class 0 to measure percentage of correctly predicted fatal casualties, confusion matrix, classification report and roc auc score to measure how good prediction overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "kappa=make_scorer(cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_eval1(model, test_X, test_y):\n",
    "    y_pred = model.predict(test_X)\n",
    "    print(\"Percentage of correct non-fatal outcome: \",round(recall_score(test_y,y_pred,pos_label=0),4)*100,\"%\")\n",
    "    print(\"Percentage of correct fatal outcome: \",round(recall_score(test_y,y_pred),4)*100,\"%\")\n",
    "    print(\"roc_auc_score: \",round(roc_auc_score(test_y,y_pred),4)*100,\"%\")\n",
    "    print(\"------------Confusion matrix----------------------\")\n",
    "    print(confusion_matrix(test_y, y_pred,labels=[0,1]))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"---------------------------------Classification report-----------------------------\")\n",
    "    print(classification_report_imbalanced(test_y, y_pred))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Cohen kappa score: \",round(cohen_kappa_score(test_y, y_pred,labels=[0,1])*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct non-fatal outcome:  48.03 %\n",
      "Percentage of correct fatal outcome:  95.56 %\n",
      "roc_auc_score:  71.78999999999999 %\n",
      "------------Confusion matrix----------------------\n",
      "[[1463 1583]\n",
      " [   2   43]]\n",
      "--------------------------------------------------\n",
      "---------------------------------Classification report-----------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      0.48      0.96      0.65      0.68      0.44      3046\n",
      "          1       0.03      0.96      0.48      0.05      0.68      0.48        45\n",
      "\n",
      "avg / total       0.98      0.49      0.95      0.64      0.68      0.44      3091\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Cohen kappa score:  2.38 %\n"
     ]
    }
   ],
   "source": [
    "performance_eval1(log_reg_down_sampling,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct non-fatal outcome:  84.31 %\n",
      "Percentage of correct fatal outcome:  62.22 %\n",
      "roc_auc_score:  73.26 %\n",
      "------------Confusion matrix----------------------\n",
      "[[2568  478]\n",
      " [  17   28]]\n",
      "--------------------------------------------------\n",
      "---------------------------------Classification report-----------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.99      0.84      0.62      0.91      0.72      0.54      3046\n",
      "          1       0.06      0.62      0.84      0.10      0.72      0.51        45\n",
      "\n",
      "avg / total       0.98      0.84      0.63      0.90      0.72      0.54      3091\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Cohen kappa score:  7.7 %\n"
     ]
    }
   ],
   "source": [
    "performance_eval1(log_reg_up_sampling,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct non-fatal outcome:  52.76 %\n",
      "Percentage of correct fatal outcome:  86.67 %\n",
      "roc_auc_score:  69.71000000000001 %\n",
      "------------Confusion matrix----------------------\n",
      "[[1607 1439]\n",
      " [   6   39]]\n",
      "--------------------------------------------------\n",
      "---------------------------------Classification report-----------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      0.53      0.87      0.69      0.68      0.44      3046\n",
      "          1       0.03      0.87      0.53      0.05      0.68      0.47        45\n",
      "\n",
      "avg / total       0.98      0.53      0.86      0.68      0.68      0.44      3091\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Cohen kappa score:  2.36 %\n"
     ]
    }
   ],
   "source": [
    "performance_eval1(mlp_down_sampling,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct non-fatal outcome:  99.08 %\n",
      "Percentage of correct fatal outcome:  11.110000000000001 %\n",
      "roc_auc_score:  55.1 %\n",
      "------------Confusion matrix----------------------\n",
      "[[3018   28]\n",
      " [  40    5]]\n",
      "--------------------------------------------------\n",
      "---------------------------------Classification report-----------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.99      0.99      0.11      0.99      0.33      0.12      3046\n",
      "          1       0.15      0.11      0.99      0.13      0.33      0.10        45\n",
      "\n",
      "avg / total       0.97      0.98      0.12      0.98      0.33      0.12      3091\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Cohen kappa score:  11.73 %\n"
     ]
    }
   ],
   "source": [
    "performance_eval1(mlp_up_sampling,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct non-fatal outcome:  82.11 %\n",
      "Percentage of correct fatal outcome:  77.78 %\n",
      "roc_auc_score:  79.94 %\n",
      "------------Confusion matrix----------------------\n",
      "[[2501  545]\n",
      " [  10   35]]\n",
      "--------------------------------------------------\n",
      "---------------------------------Classification report-----------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      0.82      0.78      0.90      0.80      0.64      3046\n",
      "          1       0.06      0.78      0.82      0.11      0.80      0.64        45\n",
      "\n",
      "avg / total       0.98      0.82      0.78      0.89      0.80      0.64      3091\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "Cohen kappa score:  8.73 %\n"
     ]
    }
   ],
   "source": [
    "performance_eval1(rus_clf,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Discussion\n",
    "\n",
    "**Test performance**\n",
    "\n",
    "| Model         | Resampling | Percentage of correct non-fatal outcome |Percentage of correct fatal outcome|  roc auc  | kappa  |\n",
    "|---------------|----------|-----------------------------------------|-----------------------------------|-------    |-----   |\n",
    "|   Logistic    |   down   |                  48.03%                 |                95.56%             |   71.79%  |  2.38% |\n",
    "|   Logistic    |    up    |                  84.31%                 |                62.22%             |   73.26%  |  7.70% |\n",
    "|      MLP      |   down   |                  52.76%                 |                86.67%             |   69.71%  |  2.36% |\n",
    "|      MLP      |    up    |                  99.08%                 |                11.11%             |   55.11%  | 11.73% |\n",
    "|  RusBoosting  |   down   |                  82.11%                 |                77.78%             |   79.94%  |  8.73% |\n",
    "\n",
    "Although mlp classifier with upsampling's kappa score is 11.73%, it poorly predicted (11.11%) 5 out of 45 fatal causalties for test set. This is due to the extremely imbalanced classes and high prediction on non-fatal casualtes. Logistic regression implemented with downsampling can correctly predict 43 out of 45 fatal casualties, but it has very high type 1 error, as percentage of correctly predicted non-fatal outcome is only 48.03 %.\n",
    "\n",
    "The most desirable model is the RUSBoosting Classifier as percentage of correctly predicted fatal outcome is 77.78% and percentage of correctly predicted non-fatal outcome is 82.11%. Its roc aus score is 79.94%, suggesting the percentage of type 1 and type 2 errors are the lowest out of all models in previous section.\n",
    "\n",
    "One notable result is that rus boosting classifier has very low kappa score. According to [score magnitude guideline](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Significance_and_magnitude), kappa score lower than 20% suggests no agreements between observed accuracy and expected accuracy. This model would be highly encouraged to only explore the relationship between considered features and the label outcome, instead of being used as a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model Intepretation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(rus_clf.best_estimator_.named_steps['rus'].feature_importances_*100,index=X.columns,columns=['Ranking (%)'])\n",
    "feature_imp['Total count'] = data.sum()\n",
    "feature_imp.loc[feature_imp['Total count']>12361,'Total count']=12321\n",
    "feature_imp['Total count']= feature_imp['Total count'].astype(int)\n",
    "feature_imp = feature_imp.sort_values(by='Ranking (%)',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking (%)</th>\n",
       "      <th>Total count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cas_age</th>\n",
       "      <td>26.00</td>\n",
       "      <td>12321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_speed</th>\n",
       "      <td>8.67</td>\n",
       "      <td>12321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crash_type_Head On</th>\n",
       "      <td>7.67</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crash_type_Hit Parked Vehicle</th>\n",
       "      <td>4.67</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_movement_Crossing without Control</th>\n",
       "      <td>4.00</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crash_type_Hit Fixed Object</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lic_type_Unlicenced</th>\n",
       "      <td>2.67</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cas_gender_Male</th>\n",
       "      <td>2.67</td>\n",
       "      <td>6197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrown_out_Thrown Out</th>\n",
       "      <td>2.67</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cas_total</th>\n",
       "      <td>2.33</td>\n",
       "      <td>12321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Ranking (%)  Total count\n",
       "cas_age                                       26.00        12321\n",
       "area_speed                                     8.67        12321\n",
       "crash_type_Head On                             7.67          493\n",
       "crash_type_Hit Parked Vehicle                  4.67          459\n",
       "unit_movement_Crossing without Control         4.00          276\n",
       "crash_type_Hit Fixed Object                    3.00         1773\n",
       "lic_type_Unlicenced                            2.67          171\n",
       "cas_gender_Male                                2.67         6197\n",
       "thrown_out_Thrown Out                          2.67          110\n",
       "cas_total                                      2.33        12321"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(feature_imp[:10],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Discussion\n",
    "\n",
    "The table above shows top 10 features found to be significant in the selected model. With use of visual aids in Data visualisation 03 notebook, we can summarise the effect of these features. \n",
    "\n",
    "Causalties with older age and higher total number of casualties taken place at highly speedy area pose higher risk of being fatal in road accidents. Male casualties associated with Head-on collision, hitted fixed object incease the chance of being killed in road accidents.Causalties associated with hitting Parked Vehicle incease the change being survived in road accidents. \n",
    "\n",
    "unit_movement_Crossing without Control, lic_type_Unlicenced and thrown_out_Thrown Out are found to have significant effect but their sample sizes are small, suggesting that these variables might not be able to generalise claimed effect on fatality outcome. \n",
    "\n",
    "Causalty taken place in country was expected to have higher significant ranking percentage but it was found to have only to have only 1% significant score. This result might be due to impact of highly significant factors such as age and area speed which can capture more variability of the label outcome. \n",
    "\n",
    "However, Age has the significant score of 26%, which might be contradicted to what we expected as younger drivers are less experienced than older drivers in reality. The result might also be affected if the drivers have driven for longer distance with longer hours. Unknown factors such as health conditions might explain why older causalties have higher risk of being fatal in accident. External factors such as drinking driving could have substantial impact on the modelling outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Summary and development for future analysis\n",
    "\n",
    "RusBoosting classifier was proven to be the best valid model, and age, area speed accident associating with head On collision, and Hitting parked Vehicle were found to be the most significant features to explain the fatality outcome in road accident. However, the low kappa score of selected model does not reflect adequate agreements between observed accuracy and expected accuracy, additional features might need to be added in follow-up analysis to increase more of variability of the data. For example, a measurement of health condition of the causalties can highly influence the outcome of fatality in road accident, as people with heart strokes might be more vulnerable to injury in road accidents, and health history can be added to the model in future analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
